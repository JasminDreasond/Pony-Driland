Image Width: {
  "value": 1024,
  "description": "1024px"
}
Image Height: {
  "value": 1024,
  "description": "1024px"
}
Bit Depth: {
  "value": 8,
  "description": "8"
}
Color Type: {
  "value": 2,
  "description": "RGB"
}
Compression: {
  "value": 0,
  "description": "Deflate/Inflate"
}
Filter: {
  "value": 0,
  "description": "Adaptive"
}
Interlace: {
  "value": 0,
  "description": "Noninterlaced"
}
prompt: {
  "value": "{\"3\": {\"inputs\": {\"seed\": 974586622738108, \"steps\": 35, \"cfg\": 7.0, \"sampler_name\": \"euler_ancestral\", \"scheduler\": \"normal\", \"denoise\": 1.0, \"model\": [\"15\", 0], \"positive\": [\"6\", 0], \"negative\": [\"31\", 0], \"latent_image\": [\"36\", 0]}, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"KSampler\"}}, \"4\": {\"inputs\": {\"ckpt_name\": \"oneFORALLPonyFantasy_v20DPO.safetensors\"}, \"class_type\": \"CheckpointLoaderSimple\", \"_meta\": {\"title\": \"Load Checkpoint\"}}, \"6\": {\"inputs\": {\"text\": \"queen chrysalis, pony, solo, cute, feral, smirking look, (anatomically correct), female, looking at viewer, tongue, wings, open mouth, high angle, uvula, white background\", \"clip\": [\"15\", 1]}, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Prompt)\"}}, \"9\": {\"inputs\": {\"filename_prefix\": \"ComfyUI\", \"images\": [\"47\", 0]}, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"}}, \"15\": {\"inputs\": {\"lora_name\": \"BoldCAT_P3.safetensors\", \"strength_model\": 1.0, \"strength_clip\": 1.0, \"model\": [\"4\", 0], \"clip\": [\"4\", 1]}, \"class_type\": \"LoraLoader\", \"_meta\": {\"title\": \"Load LoRA\"}}, \"31\": {\"inputs\": {\"text\": \"\", \"clip\": [\"4\", 1]}, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Prompt)\"}}, \"36\": {\"inputs\": {\"width\": 1024, \"height\": 1024, \"batch_size\": 1}, \"class_type\": \"EmptyLatentImage\", \"_meta\": {\"title\": \"Empty Latent Image\"}}, \"47\": {\"inputs\": {\"samples\": [\"3\", 0], \"vae\": [\"4\", 2]}, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"}}}",
  "description": "{\"3\": {\"inputs\": {\"seed\": 974586622738108, \"steps\": 35, \"cfg\": 7.0, \"sampler_name\": \"euler_ancestral\", \"scheduler\": \"normal\", \"denoise\": 1.0, \"model\": [\"15\", 0], \"positive\": [\"6\", 0], \"negative\": [\"31\", 0], \"latent_image\": [\"36\", 0]}, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"KSampler\"}}, \"4\": {\"inputs\": {\"ckpt_name\": \"oneFORALLPonyFantasy_v20DPO.safetensors\"}, \"class_type\": \"CheckpointLoaderSimple\", \"_meta\": {\"title\": \"Load Checkpoint\"}}, \"6\": {\"inputs\": {\"text\": \"queen chrysalis, pony, solo, cute, feral, smirking look, (anatomically correct), female, looking at viewer, tongue, wings, open mouth, high angle, uvula, white background\", \"clip\": [\"15\", 1]}, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Prompt)\"}}, \"9\": {\"inputs\": {\"filename_prefix\": \"ComfyUI\", \"images\": [\"47\", 0]}, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"}}, \"15\": {\"inputs\": {\"lora_name\": \"BoldCAT_P3.safetensors\", \"strength_model\": 1.0, \"strength_clip\": 1.0, \"model\": [\"4\", 0], \"clip\": [\"4\", 1]}, \"class_type\": \"LoraLoader\", \"_meta\": {\"title\": \"Load LoRA\"}}, \"31\": {\"inputs\": {\"text\": \"\", \"clip\": [\"4\", 1]}, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Prompt)\"}}, \"36\": {\"inputs\": {\"width\": 1024, \"height\": 1024, \"batch_size\": 1}, \"class_type\": \"EmptyLatentImage\", \"_meta\": {\"title\": \"Empty Latent Image\"}}, \"47\": {\"inputs\": {\"samples\": [\"3\", 0], \"vae\": [\"4\", 2]}, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"}}}"
}
workflow: {
  "value": "{\"last_node_id\": 51, \"last_link_id\": 63, \"nodes\": [{\"id\": 4, \"type\": \"CheckpointLoaderSimple\", \"pos\": [-2983.663818359375, 497.5243835449219], \"size\": [315, 98], \"flags\": {\"pinned\": true}, \"order\": 0, \"mode\": 0, \"inputs\": [], \"outputs\": [{\"name\": \"MODEL\", \"type\": \"MODEL\", \"links\": [17], \"slot_index\": 0}, {\"name\": \"CLIP\", \"type\": \"CLIP\", \"links\": [16, 63], \"slot_index\": 1}, {\"name\": \"VAE\", \"type\": \"VAE\", \"links\": [46], \"slot_index\": 2}], \"properties\": {\"Node name for S&R\": \"CheckpointLoaderSimple\"}, \"widgets_values\": [\"oneFORALLPonyFantasy_v20DPO.safetensors\"]}, {\"id\": 9, \"type\": \"SaveImage\", \"pos\": [-2597.854736328125, -471.6347351074219], \"size\": [1041.8900146484375, 703.0499877929688], \"flags\": {\"pinned\": true}, \"order\": 12, \"mode\": 0, \"inputs\": [{\"name\": \"images\", \"type\": \"IMAGE\", \"link\": 62}], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"ComfyUI\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 36, \"type\": \"EmptyLatentImage\", \"pos\": [-1513.3345947265625, -417.9139709472656], \"size\": [398.8529968261719, 112.65499877929688], \"flags\": {\"pinned\": true}, \"order\": 1, \"mode\": 0, \"inputs\": [], \"outputs\": [{\"name\": \"LATENT\", \"type\": \"LATENT\", \"links\": [37], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"EmptyLatentImage\"}, \"widgets_values\": [1024, 1024, 1], \"color\": \"#233\", \"bgcolor\": \"#355\"}, {\"id\": 3, \"type\": \"KSampler\", \"pos\": [-1047.60791015625, -358.2300109863281], \"size\": [290.3600158691406, 474], \"flags\": {\"pinned\": true}, \"order\": 10, \"mode\": 0, \"inputs\": [{\"name\": \"model\", \"type\": \"MODEL\", \"link\": 18}, {\"name\": \"positive\", \"type\": \"CONDITIONING\", \"link\": 4}, {\"name\": \"negative\", \"type\": \"CONDITIONING\", \"link\": 27}, {\"name\": \"latent_image\", \"type\": \"LATENT\", \"link\": 37}], \"outputs\": [{\"name\": \"LATENT\", \"type\": \"LATENT\", \"links\": [45], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"KSampler\"}, \"widgets_values\": [974586622738108, \"randomize\", 35, 7, \"euler_ancestral\", \"normal\", 1], \"color\": \"#233\", \"bgcolor\": \"#355\"}, {\"id\": 15, \"type\": \"LoraLoader\", \"pos\": [-2582.717041015625, 479.6656799316406], \"size\": [320.3240051269531, 126], \"flags\": {\"pinned\": true}, \"order\": 7, \"mode\": 0, \"inputs\": [{\"name\": \"model\", \"type\": \"MODEL\", \"link\": 17}, {\"name\": \"clip\", \"type\": \"CLIP\", \"link\": 16}], \"outputs\": [{\"name\": \"MODEL\", \"type\": \"MODEL\", \"links\": [18], \"slot_index\": 0}, {\"name\": \"CLIP\", \"type\": \"CLIP\", \"links\": [19], \"slot_index\": 1}], \"properties\": {\"Node name for S&R\": \"LoraLoader\"}, \"widgets_values\": [\"BoldCAT_P3.safetensors\", 1, 1]}, {\"id\": 45, \"type\": \"Note\", \"pos\": [-1261.60107421875, 639.310791015625], \"size\": [210, 309.6290588378906], \"flags\": {\"pinned\": true}, \"order\": 2, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Platypus: For the record, these are the colors it can choose from:\\nbeige\\nblack\\nblue\\nbrown\\ncyan\\ngold\\ngray\\ngreen\\nindigo\\nmagenta\\norange\\npink\\npurple\\nred\\nsilver\\nturquoise\\nviolet\\nwhite\\nyellow\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 44, \"type\": \"Note\", \"pos\": [-1492.07275390625, 632.700439453125], \"size\": [210, 313.915771484375], \"flags\": {\"pinned\": true}, \"order\": 3, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Scarlet Ribbon:\\ncolor-select\\naqua\\nred\\nblue\\nyellow\\ngreen\\npurple\\norange\\npink\\nblack\\nwhite\\ngrey\\nmaroon\\ncrimson\\nscarlet\\nburgundy\\nnavy blue\\nturquoise\\nsky blue\\nazure\\nteal\\nemerald green\\nchartreuse\\njade green\\ngold-colored\\nmustard-colored\\ncanary yellow\\nyellowish-brown\\nlavender\\nviolet\\nmauve\\nslate grey\\nsilver-colored\\ndove-grey\\nashen-colored\\nchestnut-colored\\nbeige\\nchocolate-colored\\nhazel-colored\\number\\nfuchsia\\nolive-colored\\nrainbow\\n\\nOf course, you have to be careful with some color terms cuz the AI will try and treat 'orange' as 'draw oranges'.\\nI've removed the most egregiously problematic ones\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 43, \"type\": \"Note\", \"pos\": [-1368.84716796875, -606.4607543945312], \"size\": [676.3900146484375, 85.5], \"flags\": {}, \"order\": 4, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Scarlet Ribbon: I also strongly recommend grabbing rgthree's custom nodes, if for nothing else than that the LoRA Power Loader is BY FAR the best LoRA management node.\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 41, \"type\": \"Note\", \"pos\": [-647.6083374023438, -632.6937866210938], \"size\": [625.1893920898438, 129.4781951904297], \"flags\": {}, \"order\": 5, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Scarlet Ribbon: Which means the further you go past 1024x1024, the more weird shit you're gonna get.\\nIt helps immensely to generate at a smaller resolution for a few steps, then upscale the latent to your target resolution\\nBecause the smaller resolution image will be within the 1024x1024 limitation\\nAnd then the result of that generation will be used as the 'noise' for the larger generation, and will define most of the body structure\"], \"color\": \"#332922\", \"bgcolor\": \"#593930\"}, {\"id\": 47, \"type\": \"VAEDecode\", \"pos\": [-529.0452270507812, 377.753662109375], \"size\": [210, 46], \"flags\": {}, \"order\": 11, \"mode\": 0, \"inputs\": [{\"name\": \"samples\", \"type\": \"LATENT\", \"link\": 45}, {\"name\": \"vae\", \"type\": \"VAE\", \"link\": 46}], \"outputs\": [{\"name\": \"IMAGE\", \"type\": \"IMAGE\", \"links\": [62], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"VAEDecode\"}, \"widgets_values\": [], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 40, \"type\": \"Note\", \"pos\": [-651.372314453125, 237.13600158691406], \"size\": [450.82843017578125, 92.21019744873047], \"flags\": {\"pinned\": true}, \"order\": 6, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Normal method: connect the VAE Decode to the Save Image and be happy.\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 6, \"type\": \"CLIPTextEncode\", \"pos\": [-1533.97802734375, -244.34820556640625], \"size\": [440.9950256347656, 186.0930633544922], \"flags\": {\"pinned\": true}, \"order\": 9, \"mode\": 0, \"inputs\": [{\"name\": \"clip\", \"type\": \"CLIP\", \"link\": 19}], \"outputs\": [{\"name\": \"CONDITIONING\", \"type\": \"CONDITIONING\", \"links\": [4], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"CLIPTextEncode\"}, \"widgets_values\": [\"queen chrysalis, pony, solo, cute, feral, smirking look, (anatomically correct), female, looking at viewer, tongue, wings, open mouth, high angle, uvula, white background\"], \"color\": \"#233\", \"bgcolor\": \"#355\"}, {\"id\": 31, \"type\": \"CLIPTextEncode\", \"pos\": [-1529.06201171875, 3.176849365234375], \"size\": [437.89501953125, 209.3030548095703], \"flags\": {\"pinned\": true}, \"order\": 8, \"mode\": 0, \"inputs\": [{\"name\": \"clip\", \"type\": \"CLIP\", \"link\": 63}], \"outputs\": [{\"name\": \"CONDITIONING\", \"type\": \"CONDITIONING\", \"links\": [27], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"CLIPTextEncode\"}, \"widgets_values\": [\"\"], \"color\": \"#233\", \"bgcolor\": \"#355\"}], \"links\": [[4, 6, 0, 3, 1, \"CONDITIONING\"], [16, 4, 1, 15, 1, \"CLIP\"], [17, 4, 0, 15, 0, \"MODEL\"], [18, 15, 0, 3, 0, \"MODEL\"], [19, 15, 1, 6, 0, \"CLIP\"], [27, 31, 0, 3, 2, \"CONDITIONING\"], [37, 36, 0, 3, 3, \"LATENT\"], [45, 3, 0, 47, 0, \"LATENT\"], [46, 4, 2, 47, 1, \"VAE\"], [62, 47, 0, 9, 0, \"IMAGE\"], [63, 4, 1, 31, 0, \"CLIP\"]], \"groups\": [], \"config\": {}, \"extra\": {\"ds\": {\"scale\": 0.9090909090909091, \"offset\": [3078.175530626775, 395.0211721106156]}}, \"version\": 0.4}",
  "description": "{\"last_node_id\": 51, \"last_link_id\": 63, \"nodes\": [{\"id\": 4, \"type\": \"CheckpointLoaderSimple\", \"pos\": [-2983.663818359375, 497.5243835449219], \"size\": [315, 98], \"flags\": {\"pinned\": true}, \"order\": 0, \"mode\": 0, \"inputs\": [], \"outputs\": [{\"name\": \"MODEL\", \"type\": \"MODEL\", \"links\": [17], \"slot_index\": 0}, {\"name\": \"CLIP\", \"type\": \"CLIP\", \"links\": [16, 63], \"slot_index\": 1}, {\"name\": \"VAE\", \"type\": \"VAE\", \"links\": [46], \"slot_index\": 2}], \"properties\": {\"Node name for S&R\": \"CheckpointLoaderSimple\"}, \"widgets_values\": [\"oneFORALLPonyFantasy_v20DPO.safetensors\"]}, {\"id\": 9, \"type\": \"SaveImage\", \"pos\": [-2597.854736328125, -471.6347351074219], \"size\": [1041.8900146484375, 703.0499877929688], \"flags\": {\"pinned\": true}, \"order\": 12, \"mode\": 0, \"inputs\": [{\"name\": \"images\", \"type\": \"IMAGE\", \"link\": 62}], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"ComfyUI\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 36, \"type\": \"EmptyLatentImage\", \"pos\": [-1513.3345947265625, -417.9139709472656], \"size\": [398.8529968261719, 112.65499877929688], \"flags\": {\"pinned\": true}, \"order\": 1, \"mode\": 0, \"inputs\": [], \"outputs\": [{\"name\": \"LATENT\", \"type\": \"LATENT\", \"links\": [37], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"EmptyLatentImage\"}, \"widgets_values\": [1024, 1024, 1], \"color\": \"#233\", \"bgcolor\": \"#355\"}, {\"id\": 3, \"type\": \"KSampler\", \"pos\": [-1047.60791015625, -358.2300109863281], \"size\": [290.3600158691406, 474], \"flags\": {\"pinned\": true}, \"order\": 10, \"mode\": 0, \"inputs\": [{\"name\": \"model\", \"type\": \"MODEL\", \"link\": 18}, {\"name\": \"positive\", \"type\": \"CONDITIONING\", \"link\": 4}, {\"name\": \"negative\", \"type\": \"CONDITIONING\", \"link\": 27}, {\"name\": \"latent_image\", \"type\": \"LATENT\", \"link\": 37}], \"outputs\": [{\"name\": \"LATENT\", \"type\": \"LATENT\", \"links\": [45], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"KSampler\"}, \"widgets_values\": [974586622738108, \"randomize\", 35, 7, \"euler_ancestral\", \"normal\", 1], \"color\": \"#233\", \"bgcolor\": \"#355\"}, {\"id\": 15, \"type\": \"LoraLoader\", \"pos\": [-2582.717041015625, 479.6656799316406], \"size\": [320.3240051269531, 126], \"flags\": {\"pinned\": true}, \"order\": 7, \"mode\": 0, \"inputs\": [{\"name\": \"model\", \"type\": \"MODEL\", \"link\": 17}, {\"name\": \"clip\", \"type\": \"CLIP\", \"link\": 16}], \"outputs\": [{\"name\": \"MODEL\", \"type\": \"MODEL\", \"links\": [18], \"slot_index\": 0}, {\"name\": \"CLIP\", \"type\": \"CLIP\", \"links\": [19], \"slot_index\": 1}], \"properties\": {\"Node name for S&R\": \"LoraLoader\"}, \"widgets_values\": [\"BoldCAT_P3.safetensors\", 1, 1]}, {\"id\": 45, \"type\": \"Note\", \"pos\": [-1261.60107421875, 639.310791015625], \"size\": [210, 309.6290588378906], \"flags\": {\"pinned\": true}, \"order\": 2, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Platypus: For the record, these are the colors it can choose from:\\nbeige\\nblack\\nblue\\nbrown\\ncyan\\ngold\\ngray\\ngreen\\nindigo\\nmagenta\\norange\\npink\\npurple\\nred\\nsilver\\nturquoise\\nviolet\\nwhite\\nyellow\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 44, \"type\": \"Note\", \"pos\": [-1492.07275390625, 632.700439453125], \"size\": [210, 313.915771484375], \"flags\": {\"pinned\": true}, \"order\": 3, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Scarlet Ribbon:\\ncolor-select\\naqua\\nred\\nblue\\nyellow\\ngreen\\npurple\\norange\\npink\\nblack\\nwhite\\ngrey\\nmaroon\\ncrimson\\nscarlet\\nburgundy\\nnavy blue\\nturquoise\\nsky blue\\nazure\\nteal\\nemerald green\\nchartreuse\\njade green\\ngold-colored\\nmustard-colored\\ncanary yellow\\nyellowish-brown\\nlavender\\nviolet\\nmauve\\nslate grey\\nsilver-colored\\ndove-grey\\nashen-colored\\nchestnut-colored\\nbeige\\nchocolate-colored\\nhazel-colored\\number\\nfuchsia\\nolive-colored\\nrainbow\\n\\nOf course, you have to be careful with some color terms cuz the AI will try and treat 'orange' as 'draw oranges'.\\nI've removed the most egregiously problematic ones\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 43, \"type\": \"Note\", \"pos\": [-1368.84716796875, -606.4607543945312], \"size\": [676.3900146484375, 85.5], \"flags\": {}, \"order\": 4, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Scarlet Ribbon: I also strongly recommend grabbing rgthree's custom nodes, if for nothing else than that the LoRA Power Loader is BY FAR the best LoRA management node.\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 41, \"type\": \"Note\", \"pos\": [-647.6083374023438, -632.6937866210938], \"size\": [625.1893920898438, 129.4781951904297], \"flags\": {}, \"order\": 5, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Scarlet Ribbon: Which means the further you go past 1024x1024, the more weird shit you're gonna get.\\nIt helps immensely to generate at a smaller resolution for a few steps, then upscale the latent to your target resolution\\nBecause the smaller resolution image will be within the 1024x1024 limitation\\nAnd then the result of that generation will be used as the 'noise' for the larger generation, and will define most of the body structure\"], \"color\": \"#332922\", \"bgcolor\": \"#593930\"}, {\"id\": 47, \"type\": \"VAEDecode\", \"pos\": [-529.0452270507812, 377.753662109375], \"size\": [210, 46], \"flags\": {}, \"order\": 11, \"mode\": 0, \"inputs\": [{\"name\": \"samples\", \"type\": \"LATENT\", \"link\": 45}, {\"name\": \"vae\", \"type\": \"VAE\", \"link\": 46}], \"outputs\": [{\"name\": \"IMAGE\", \"type\": \"IMAGE\", \"links\": [62], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"VAEDecode\"}, \"widgets_values\": [], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 40, \"type\": \"Note\", \"pos\": [-651.372314453125, 237.13600158691406], \"size\": [450.82843017578125, 92.21019744873047], \"flags\": {\"pinned\": true}, \"order\": 6, \"mode\": 0, \"inputs\": [], \"outputs\": [], \"properties\": {}, \"widgets_values\": [\"Normal method: connect the VAE Decode to the Save Image and be happy.\"], \"color\": \"#222\", \"bgcolor\": \"#000\"}, {\"id\": 6, \"type\": \"CLIPTextEncode\", \"pos\": [-1533.97802734375, -244.34820556640625], \"size\": [440.9950256347656, 186.0930633544922], \"flags\": {\"pinned\": true}, \"order\": 9, \"mode\": 0, \"inputs\": [{\"name\": \"clip\", \"type\": \"CLIP\", \"link\": 19}], \"outputs\": [{\"name\": \"CONDITIONING\", \"type\": \"CONDITIONING\", \"links\": [4], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"CLIPTextEncode\"}, \"widgets_values\": [\"queen chrysalis, pony, solo, cute, feral, smirking look, (anatomically correct), female, looking at viewer, tongue, wings, open mouth, high angle, uvula, white background\"], \"color\": \"#233\", \"bgcolor\": \"#355\"}, {\"id\": 31, \"type\": \"CLIPTextEncode\", \"pos\": [-1529.06201171875, 3.176849365234375], \"size\": [437.89501953125, 209.3030548095703], \"flags\": {\"pinned\": true}, \"order\": 8, \"mode\": 0, \"inputs\": [{\"name\": \"clip\", \"type\": \"CLIP\", \"link\": 63}], \"outputs\": [{\"name\": \"CONDITIONING\", \"type\": \"CONDITIONING\", \"links\": [27], \"slot_index\": 0}], \"properties\": {\"Node name for S&R\": \"CLIPTextEncode\"}, \"widgets_values\": [\"\"], \"color\": \"#233\", \"bgcolor\": \"#355\"}], \"links\": [[4, 6, 0, 3, 1, \"CONDITIONING\"], [16, 4, 1, 15, 1, \"CLIP\"], [17, 4, 0, 15, 0, \"MODEL\"], [18, 15, 0, 3, 0, \"MODEL\"], [19, 15, 1, 6, 0, \"CLIP\"], [27, 31, 0, 3, 2, \"CONDITIONING\"], [37, 36, 0, 3, 3, \"LATENT\"], [45, 3, 0, 47, 0, \"LATENT\"], [46, 4, 2, 47, 1, \"VAE\"], [62, 47, 0, 9, 0, \"IMAGE\"], [63, 4, 1, 31, 0, \"CLIP\"]], \"groups\": [], \"config\": {}, \"extra\": {\"ds\": {\"scale\": 0.9090909090909091, \"offset\": [3078.175530626775, 395.0211721106156]}}, \"version\": 0.4}"
}
FileType: {
  "value": "png",
  "description": "PNG"
}